{
  "cells": [
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "N2O8UuGAxZhW"
      },
      "source": [
        "# GAN implementation in python\n",
        "| Group members | Registration Number |\n",
        "|---|:---:|\n",
        "| Yashvardhan Singh | 20BAI10135 |\n",
        "| Aditya Lanka | 20BAI10236 |\n",
        "| Siddhartha Maratha | 20BAI10257 |\n",
        "| Anshumann Ravichandar | 20BAI10281 |\n",
        "| Mukul Sharma | 20BAI10222 |"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "RMkjAdXQxW1v"
      },
      "source": [
        "### Setup"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "H88fNCXQwTsr"
      },
      "outputs": [],
      "source": [
        "!pip install torch"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "Onea4skFxNSE"
      },
      "source": [
        "### Preparing training data\n",
        "The training data is composed of pairs (x<sub>1</sub>, x<sub>2</sub>) so that x<sub>2</sub> consists of the value of the sine of x<sub>1</sub> for x<sub>2</sub> in the interval from 0 to 2π \\\n",
        "(0 to 2π so that the sin wave is complete)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Rj1N4Z6WxCR4"
      },
      "outputs": [],
      "source": [
        "import math\n",
        "import torch\n",
        "from torch import nn\n",
        "\n",
        "torch.manual_seed(111)\n",
        "train_data_length = 1024\n",
        "\n",
        "train_data = torch.zeros((train_data_length, 2))\n",
        "train_data[:, 0] = 2 * math.pi * torch.rand(train_data_length)\n",
        "train_data[:, 1] = torch.sin(train_data[:, 0])\n",
        "print(train_data)\n",
        "\n",
        "train_labels = torch.zeros(train_data_length)\n",
        "train_set = [(train_data[i], train_labels[i]) for i in range(train_data_length)]\n",
        "print(train_set)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "17BWoGd4xKMO"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "plt.plot(train_data[:, 0], train_data[:, 1], \".\")"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "Y-PZYxjm1u1s"
      },
      "source": [
        "### Preparing training and testing samples for the models"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JTlDMGiTy_NW"
      },
      "outputs": [],
      "source": [
        "batch_size = 32 # we will train the network in batches of data\n",
        "train_loader = torch.utils.data.DataLoader(train_set, batch_size=batch_size, shuffle=True) # Training Data\n",
        "print('Training Batches:')\n",
        "for batch_idx, sample in enumerate(train_loader):\n",
        "  if batch_idx == 2:\n",
        "    print('...')\n",
        "    break\n",
        "  print(batch_idx, sample)\n",
        "\n",
        "print('Testing Data:')\n",
        "latent_space_samples = torch.randn(train_data_length, 2) # Testing Data\n",
        "print(latent_space_samples)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "72QMjKR5ztsP"
      },
      "source": [
        "### Implementing the Discriminator\n",
        "- **nn.Linear(n,m)** - is a module that creates single layer feed forward network with `n` inputs and `m` output. Mathematically, this module is designed to calculate the linear equation `Ax = b` where x is input tensor, b is output, A is weight matrix.\\\n",
        "<img src='./pytorch_nn_linear_fn.png' alt='Neural network with 3 inputs connected to 2 nodes to generate 2 outputs'></img>\n",
        "- **nn.ReLu()** - applies the rectified linear unit function element-wise on an *input tensor*.\n",
        "- **nn.Dropout(p)** - During training, randomly zeroes some of the elements of the input tensor with probability `p` using samples from a Bernoulli distribution. Furthermore, the outputs are scaled by a factor of `1 / (1- p)` during training.\n",
        "- **nn.Sigmoid()** - Applies the function 1 / 1 + e<sup>-x</sup> on the recieved *single input value*."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "C6be4vUlzM_4"
      },
      "outputs": [],
      "source": [
        "class Discriminator(nn.Module):\n",
        "  def __init__(self):\n",
        "    super().__init__()\n",
        "\n",
        "    self.model = nn.Sequential(\n",
        "      nn.Linear(2, 256), # layer taking vector with 2 attributes as input and map it to 256 neurons\n",
        "      nn.ReLU(), # Applies the rectified linear unit activation function\n",
        "      nn.Dropout(0.3), # Randomly drops neurons with given probability\n",
        "\n",
        "      nn.Linear(256, 128), # 256 nodes connected to 128 nodes\n",
        "      nn.ReLU(),\n",
        "      nn.Dropout(0.3),\n",
        "\n",
        "      nn.Linear(128, 64), # 128 nodes connected to 64 nodes\n",
        "      nn.ReLU(),\n",
        "      nn.Dropout(0.3),\n",
        "\n",
        "      nn.Linear(64, 1), # 64 nodes connected to 1 output node\n",
        "      nn.Sigmoid(),\n",
        "    )\n",
        "\n",
        "  def forward(self, x):\n",
        "    output = self.model(x)\n",
        "    return output\n",
        "\n",
        "discriminator = Discriminator()"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "4wP37Y0HzqQF"
      },
      "source": [
        "### Implementing the Generator\n",
        "Receives an input of tensor having 2 attributes and outputs a tensor having 2 attributes. Therefore, it maps the given input to an output in the same space, which means the network *generates* an output from a random latent space."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uxbe9d1fzlWT"
      },
      "outputs": [],
      "source": [
        "class Generator(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "\n",
        "        self.model = nn.Sequential(\n",
        "            nn.Linear(2, 16), # layer taking vector with 2 attributes as input and map it to 16 neurons\n",
        "            nn.ReLU(), # Applies the rectified linear unit activation function\n",
        "\n",
        "            nn.Linear(16, 32), # 16 nodes connected to 32 nodes\n",
        "            nn.ReLU(),\n",
        "\n",
        "            nn.Linear(32, 2), # 32 nodes connected to 2 nodes\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        output = self.model(x)\n",
        "        return output\n",
        "\n",
        "generator = Generator()"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "ed6m_2m6ACJ7"
      },
      "source": [
        "### Train the model\n",
        "- **nn.BCELoss()** - Creates a criterion that measures the Binary Cross Entropy between the target and the input probabilities. If no attribute value is given for the attribute `reduction` then the function returns the mean loss of each pair of input and target sample. (source: [PyTorch Docs > Torch.nn > BCELoss](https://pytorch.org/docs/stable/generated/torch.nn.BCELoss.html))\n",
        "- **torch.optim.Adam(...)** - performs Adam optimization on the Gradient Descent algorithm, by using momentum and RMS prop of previous epoch's gradient. You can further read from [PyTorch Docs > torch.optim > Adam](https://pytorch.org/docs/stable/generated/torch.optim.Adam.html)\n",
        "- **torch.optim.Optimizer.zero_grad()** - from [\"Why do we need to call zero_grad() in PyTorch?\" (Stackoverflow)](https://stackoverflow.com/a/48009142/16084581):\n",
        "> In `PyTorch`, for every mini-batch during the training phase, we typically want to explicitly set the gradients to zero before starting to do backpropagation (i.e., updating the Weights and biases) because PyTorch accumulates the gradients on subsequent backward passes.\\\n",
        "...\\\n",
        "Because of this, when you start your training loop, ideally you should zero out the gradients so that you do the parameter update correctly. Otherwise, the gradient would be a combination of the old gradient, which you have already used to update your model parameters and the newly-computed gradient. It would therefore point in some other direction than the intended direction towards the optimum.\\\n",
        "...\\\n",
        "The accumulation (i.e., sum) of gradients happens when `.backward()` is called on the loss tensor.\n",
        "- **torch.Tensor.backward()** - Computes the gradient of current tensor w.r.t. graph leaves (i.e. in the backward pass of the neural network training). The graph is differentiated using the chain rule.\n",
        "- **torch.optim.Optimizer.step()** - Performs a single optimization step (parameter update), which in the case of this neural network model is `Adam` optimization of `gradient descent` algorithm."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yh1lX8q9z2Yt"
      },
      "outputs": [],
      "source": [
        "from time import sleep\n",
        "from IPython.display import clear_output\n",
        "\n",
        "lr = 0.0001\n",
        "num_epochs = 500\n",
        "loss_function = nn.BCELoss()\n",
        "\n",
        "optimizer_discriminator = torch.optim.Adam(discriminator.parameters(), lr=lr)\n",
        "optimizer_generator = torch.optim.Adam(generator.parameters(), lr=lr)\n",
        "\n",
        "loss_values = \"\"\n",
        "for epoch in range(1, num_epochs + 1):\n",
        "  for n, (real_samples, _) in enumerate(train_loader):\n",
        "    # Data for training the discriminator\n",
        "    real_samples_labels = torch.ones((batch_size, 1)) # 1 - means real data\n",
        "\n",
        "    # generate samples for discriminator to discriminate\n",
        "    latent_space_samples = torch.randn((batch_size, 2))\n",
        "    generated_samples = generator(latent_space_samples)\n",
        "    generated_samples_labels = torch.zeros((batch_size, 1)) # 0 - means fake data\n",
        "\n",
        "    # concatenate all the data into a single input and target (or labels) tensor\n",
        "    all_samples = torch.cat((real_samples, generated_samples))\n",
        "    all_samples_labels = torch.cat((real_samples_labels, generated_samples_labels))\n",
        "\n",
        "    # Training the discriminator\n",
        "    discriminator.zero_grad()\n",
        "    output_discriminator = discriminator(all_samples)\n",
        "    \n",
        "    # Discriminator loss\n",
        "    loss_discriminator = loss_function(output_discriminator, all_samples_labels)\n",
        "    loss_discriminator.backward()\n",
        "    optimizer_discriminator.step()\n",
        "\n",
        "    # Data for training the generator\n",
        "    latent_space_samples = torch.randn((batch_size, 2))\n",
        "\n",
        "    # Training the generator\n",
        "    generator.zero_grad()\n",
        "    generated_samples = generator(latent_space_samples)\n",
        "    output_discriminator_generated = discriminator(generated_samples)\n",
        "\n",
        "    # Generator loss\n",
        "    loss_generator = loss_function(output_discriminator_generated, real_samples_labels)\n",
        "    loss_generator.backward()\n",
        "    optimizer_generator.step()\n",
        "\n",
        "    # Show loss and visualize the outputs\n",
        "    if (epoch % 5 == 0) and (n == batch_size - 1):\n",
        "      clear_output(wait=True)\n",
        "      loss_values += f\"Epoch: {epoch} Loss in D.: {loss_discriminator}\\n\" + f\"Epoch: {epoch} Loss in G.: {loss_generator}\\n\" + \"--------------------\\n\"\n",
        "      generated_samples = generated_samples.detach() # converts into a normal tensor (which can be converted to numpy)\n",
        "      plt.title(f\"Output of Generator at epoch: {epoch}\")\n",
        "      plt.plot(generated_samples[:, 0], generated_samples[:, 1], \".\")\n",
        "      plt.show()\n",
        "      sleep(0.1)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "zDhlyKmG0g-d"
      },
      "source": [
        "### Testing the model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "284dzWgy0jwR"
      },
      "outputs": [],
      "source": [
        "# Loss at various epochs\n",
        "print(loss_values)\n",
        "\n",
        "latent_space_samples = torch.randn(train_data_length, 2)\n",
        "generated_samples = generator(latent_space_samples)\n",
        "\n",
        "generated_samples = generated_samples.detach()\n",
        "plt.plot(generated_samples[:, 0], generated_samples[:, 1], \".\")"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "UoEWAOs5AFub"
      },
      "source": [
        "### How GANs' generator learns to mimic the real data:\n",
        "<img src=\"./GAN training 500 epochs.gif\" alt=\"GAN training on 500 epochs\"></img>"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
